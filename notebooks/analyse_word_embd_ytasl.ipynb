{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('/home/grt/youtube-asl_data/data/tsv_files/new_youtube-asl_v1_1.tsv', sep='\\t')\n",
    "\n",
    "# Load training text samples\n",
    "split = 'train'\n",
    "column = 'raw-text'\n",
    "data_frame = data_frame.loc[data_frame['split'].str.contains(split)]\n",
    "translation = data_frame[column]\n",
    "vids = data_frame['vid']\n",
    "\n",
    "# Using Punkt to tokenize words\n",
    "translation=translation.astype(str)\n",
    "sent_tks = [word_tokenize(s.lower()) for s in translation]\n",
    "tag_res = [nltk.pos_tag(tks) for tks in sent_tks]\n",
    "\n",
    "joined_tag_res = []\n",
    "for l in tag_res:\n",
    "    joined_tag_res.extend(l)\n",
    "\n",
    "freq_dist = nltk.ConditionalFreqDist(joined_tag_res)\n",
    "\n",
    "exclude_words = ['was', 'i', 'said', 'aslcaptions.com', '\\'s', 'is', 'be', 'are', 'has', 'www.aslcaptions.com', 'did', '\\'ve', '\\'m', '%', 've', 'r', 'd', '*', 'b', 'ed', 'e.', '[', ']', 'dpan.tv', 'iii', '<', '>', '/i', '']\n",
    "collect_keys = {'NN', 'NNP', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "filtered_words = {}\n",
    "for word, freq in freq_dist.items():\n",
    "    if word in exclude_words: continue\n",
    "    key_set = set(freq.keys())\n",
    "    intersect = collect_keys.intersection(key_set)\n",
    "    if len(intersect) > 0:\n",
    "        filtered_freq = {}\n",
    "        for tag in intersect:\n",
    "            if freq[tag] > 10:\n",
    "                filtered_freq[tag] = freq[tag]\n",
    "        if len(filtered_freq) > 0: \n",
    "            filtered_words[word] = filtered_freq\n",
    "\n",
    "# Load GloVe embeddings\n",
    "# vocab = []\n",
    "# embeddings = []\n",
    "# with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         items = line.strip().split(' ')\n",
    "#         vocab.append(items[0])\n",
    "#         embeddings.append(np.asarray(items[1:], 'float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('ytasl-v1.0/uncased_filtred_VNs.json', 'w') as f:\n",
    "    json.dump(filtered_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cross filter with glove vocabulary\n",
    "import numpy as np\n",
    "vocab = []\n",
    "embeddings = []\n",
    "with open('/home/grt/GloVe/glove.6B/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.strip().split(' ')\n",
    "        vocab.append(items[0])\n",
    "        embeddings.append(np.asarray(items[1:], 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_dict = json.load(open('ytasl-v1.0/uncased_filtred_VNs.json', 'r'))\n",
    "VNs = VN_dict.keys()\n",
    "\n",
    "OOV = 0\n",
    "OOV_word = []\n",
    "for vn in VNs:\n",
    "    if vn not in vocab:\n",
    "        OOV += 1\n",
    "        OOV_word.append(vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 â™ª\n"
     ]
    }
   ],
   "source": [
    "for k in OOV_word:\n",
    "    stat = VN_dict[k]\n",
    "    total = 0\n",
    "    for pos, num in stat.items():\n",
    "        total += num\n",
    "    print(total, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in OOV_word:\n",
    "    VN_dict.pop(k)\n",
    "\n",
    "with open('ytasl-v1.0/uncased_filtred_glove_VNs.json', 'w') as f:\n",
    "    json.dump(VN_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Index word mapping for glove filtered VNs\n",
    "import json\n",
    "\n",
    "with open('ytasl-v1.0/uncased_filtred_glove_VNs.json', 'r') as f:\n",
    "    vn_dict = json.load(f)\n",
    "\n",
    "vn_words = list(vn_dict.keys())\n",
    "with open('ytasl-v1.0/uncased_filtred_glove_VN_idxs.txt', 'w') as f:\n",
    "    for idx, word in enumerate(vn_words):\n",
    "        f.write(f'{idx} {word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m glove_embedding_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/grt/GloVe/glove.6B/glove.6B.300d.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         items \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m         glove_embedding_dict[items[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(items[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m glove_embedding_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/grt/GloVe/glove.6B/glove.6B.300d.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         items \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m         glove_embedding_dict[items[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(items[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/glofe/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/glofe/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate the corresponding embedding pkl\n",
    "import numpy as np\n",
    "\n",
    "vn_glove_embeddings = []\n",
    "\n",
    "glove_embedding_dict = {}\n",
    "\n",
    "with open('/home/grt/GloVe/glove.6B/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.strip().split(' ')\n",
    "        glove_embedding_dict[items[0]] = np.asarray(items[1:], 'float32')\n",
    "        \n",
    "for word in vn_words:\n",
    "    vn_glove_embeddings.append(glove_embedding_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vn_glove_embed \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvn_glove_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m vn_glove_embed\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/glofe/lib/python3.8/site-packages/numpy/core/shape_base.py:460\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    458\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    462\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "vn_glove_embed = np.stack(vn_glove_embeddings, axis=0)\n",
    "vn_glove_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('ytasl-v1.0/uncased_filtred_glove_VN_embed.pkl', 'wb') as f:\n",
    "    pkl.dump(vn_glove_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trainning infomation\n",
    "data_frame = pd.read_csv('/home/grt/youtube-asl_data/data/tsv_files/new_youtube-asl_v1_1.tsv', sep='\\t')\n",
    "\n",
    "# Load training text samples\n",
    "split = 'train'\n",
    "column = 'raw-text'\n",
    "data_frame = data_frame.loc[data_frame['split'].str.contains(split)]\n",
    "translation = data_frame[column]\n",
    "vids = data_frame['vid']\n",
    "\n",
    "VN_dict = json.load(open('ytasl-v1.0/uncased_filtred_glove_VNs.json', 'r'))\n",
    "VNs = VN_dict.keys()\n",
    "\n",
    "matched = {}\n",
    "for vid, trans in zip(vids, translation):\n",
    "    ref_word_list = word_tokenize(trans)\n",
    "    matched_words = []\n",
    "    for ref_word in ref_word_list:\n",
    "        if ref_word in VNs:\n",
    "            matched_words.append(ref_word)\n",
    "    matched[vid] = matched_words\n",
    "\n",
    "with open('ytasl-v1.0/uncased_filtred_glove_VN_matched_train.json', 'w') as f:\n",
    "    json.dump(matched, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a5ee8f268a58a1501ad7aef09cde53105f57cea18e29cd62af7d0e62261f331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
